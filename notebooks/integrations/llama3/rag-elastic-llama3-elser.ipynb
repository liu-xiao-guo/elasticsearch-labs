{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430bd0198f228af0",
   "metadata": {},
   "source": [
    "# RAG with Elastic ELSER and Llama3 using Langchain\n",
    "\n",
    "This interactive notebook uses `Langchain` to process fictional workplace documents and uses `ELSER v2` running in `Elasticsearch` to transform these documents into embeddings and store them into `Elasticsearch`. We then ask a question, retrieve the relevant documents from `Elasticsearch` and use `Llama3` running locally using `Ollama` to provide a response. \n",
    "\n",
    "**_Note_** : _`Llama3` is expected to be running using `Ollama` on the same machine where you will be running this notebook._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d3a839afa5bf1",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "For this example, you will need:\n",
    "\n",
    "- An Elastic deployment\n",
    "  - We'll be using a local Elasticsearch setup\n",
    "  - For LLM we will be using [Ollama](https://ollama.com/) and [Llama3](https://ollama.com/library/llama3) configured locally.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497ed11046b9a21",
   "metadata": {},
   "source": [
    "## Install required dependencies\n",
    "First we install the packages we need for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0c049f18215f065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: langchain in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.1.11)\n",
      "Requirement already satisfied: langchain-elasticsearch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.2.2)\n",
      "Requirement already satisfied: langchain-community in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0.29)\n",
      "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: elasticsearch[async] in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (8.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (0.1.65)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from elasticsearch[async]) (8.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from elastic-transport<9,>=8.13->elasticsearch[async]) (2.2.2)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from elastic-transport<9,>=8.13->elasticsearch[async]) (2024.2.2)\n",
      "Requirement already satisfied: simsimd>=3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (4.4.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain langchain-elasticsearch langchain-community tiktoken python-dotenv elasticsearch[async]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97921e3cf79cc71",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "Next we import the required packages as required. The imports are placed in the cells as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5be75ecc855e59b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T11:36:07.879351Z",
     "start_time": "2024-06-04T11:35:57.006555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://elastic:uK+7WbkeXMzwk9YvP-H3@localhost:9200\n",
      "{'name': 'liuxgm.local', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'coIKHIPsTf2_aWWQ8TO4bw', 'version': {'number': '8.14.1', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': '93a57a1a76f556d8aee6a90d1a95b06187501310', 'build_date': '2024-06-10T23:35:17.114581191Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, AsyncElasticsearch, helpers\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    " \n",
    "load_dotenv()\n",
    " \n",
    "ES_USER = os.getenv(\"ES_USER\")\n",
    "ES_PASSWORD = os.getenv(\"ES_PASSWORD\")\n",
    "ES_ENDPOINT = os.getenv(\"ES_ENDPOINT\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    " \n",
    "url = f\"https://{ES_USER}:{ES_PASSWORD}@{ES_ENDPOINT}:9200\"\n",
    "print(url)\n",
    " \n",
    "client = Elasticsearch(url, ca_certs = \"./http_ca.crt\", verify_certs = True)\n",
    "# info = await client.info()\n",
    "print(client.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f107432173d8df",
   "metadata": {},
   "source": [
    "## Prepare documents for chunking and ingestion\n",
    "We now prepare the data to be ingested into `Elasticsearch`. We use `LangChain`'s `RecursiveCharacterTextSplitter` and split the documents' text at 512 characters with an overlap of 256 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f949859a20b22d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T11:36:11.046835Z",
     "start_time": "2024-06-04T11:36:10.641480Z"
    }
   },
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/datasets/workplace-documents.json\"\n",
    "\n",
    "# response = urlopen(url)\n",
    "\n",
    "# workplace_docs = json.loads(response.read())\n",
    "\n",
    "# Load data into a JSON object\n",
    "with open('workplace-documents.json') as f:\n",
    "   workplace_docs = json.load(f)\n",
    "\n",
    "metadata = []\n",
    "content = []\n",
    "for doc in workplace_docs:\n",
    "    content.append(doc[\"content\"])\n",
    "    metadata.append(\n",
    "        {\n",
    "            \"name\": doc[\"name\"],\n",
    "            \"summary\": doc[\"summary\"],\n",
    "            \"rolePermissions\": doc[\"rolePermissions\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=256\n",
    ")\n",
    "docs = text_splitter.create_documents(content, metadatas=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ec8a0a663f581",
   "metadata": {},
   "source": [
    "## Define Elasticsearch Vector Store\n",
    "We define `ElasticsearchStore` as the vector store with [SparseVectorStrategy](https://python.langchain.com/v0.2/docs/integrations/vectorstores/elasticsearch/#sparsevectorstrategy-elser).`SparseVectorStrategy` converts each document into tokens and would be stored in vector field with datatype `rank_features`.\n",
    "We will be using text embedding from [ELSER v2](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html#elser-v2) model `.elser_model_2_linux`\n",
    "\n",
    "Note: Before we begin indexing, ensure you have [downloaded and deployed ELSER v2 model](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html#download-deploy-elser) in your deployment and is running in ml node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc52a85c69f6e9fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T11:36:33.046663Z",
     "start_time": "2024-06-04T11:36:32.381802Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain_elasticsearch import SparseVectorStrategy\n",
    "\n",
    "index_name = \"workplace_index_elser\"\n",
    "\n",
    "# Delete the index if it exists\n",
    "if client.indices.exists(index=index_name):\n",
    "    client.indices.delete(index=index_name)\n",
    "\n",
    "es_vector_store = ElasticsearchStore(\n",
    "    es_user = ES_USER,\n",
    "    es_password = ES_PASSWORD,\n",
    "    es_url = url,\n",
    "    es_connection = client, \n",
    "    index_name=index_name,\n",
    "    strategy=SparseVectorStrategy(model_id=\".elser_model_2\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38b91b2a358994",
   "metadata": {},
   "source": [
    "## Add docs processed above. \n",
    "The document has already been chunked. We do not use any specific embedding function here, since the tokens are inferred at index time and at query time within Elasticsearch. \n",
    "This requires that the `ELSER v2` model to be loaded and running in Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ee38329856ea47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d252429b-19ab-4761-a55a-b5523996f02c',\n",
       " '6f326aac-71ad-43d5-a0bf-4f5e81bc6dad',\n",
       " '550ef0f5-70f0-4b43-a9ff-72e2e9a34904',\n",
       " 'e11a8baf-7365-4f4b-aed0-95dfc1d58179',\n",
       " 'dfdf630a-7064-4bd4-b5d6-c78487255fe3',\n",
       " 'd445c931-7627-4c2f-a494-7015e5cf9c0c',\n",
       " '25cbdf0e-343e-4c0f-a017-a4cd09d20423',\n",
       " '610af5b2-d3b9-4bf7-bce1-82b27256cc55',\n",
       " 'bdb490de-31eb-4c7a-af8e-58a03d8d5cc0',\n",
       " '22d91725-806d-4d64-8d60-986a21b7d76d',\n",
       " 'f4a2276b-b876-4b0a-a4fd-d34b195aa881',\n",
       " '76fd4b17-7afd-4863-a30d-969068aea44a',\n",
       " 'a710cf56-b4c2-4558-af74-a7166ca5d405',\n",
       " '63fef806-1813-4a79-bdce-07b901925ce0',\n",
       " '1292541b-7aa8-404e-8333-02d1c8d32f66',\n",
       " '9a767beb-6b2e-49c2-b8d9-f9600bc3da79',\n",
       " '7536a334-2fd3-4608-b1fb-a212821b361c',\n",
       " 'eb708368-67f1-4b2d-8c40-6db7e7d7218a',\n",
       " '35af7992-4434-4a3c-9730-be25c482672c',\n",
       " '085b6138-e421-4e45-8c8f-bf14025ea832',\n",
       " '3bd9475b-1d7c-4ecb-b2c7-5099049816ec',\n",
       " '9164ab8a-e392-49f7-85e6-39946b2db501',\n",
       " '85e31f91-5d3d-4519-b731-e149743ef254',\n",
       " '83dfb216-540c-4986-b861-3d0de3b6969e',\n",
       " 'ea56915f-2224-465a-9690-d2a9dbafcd60',\n",
       " '4589474e-9eb4-4d99-8de4-c1fe14272cd5']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a628abfb6a5a",
   "metadata": {},
   "source": [
    "## LLM Configuration\n",
    "This connects to your local LLM. Please refer to https://ollama.com/library/llama3 for details on steps to run Llama3 locally. \n",
    "\n",
    "_If you have sufficient resources (atleast >64 GB Ram and GPU available) then you could try the 70B parameter version of Llama3_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23e35e152e9a2714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T11:36:40.306996Z",
     "start_time": "2024-06-04T11:36:40.302460Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca801f2aae187",
   "metadata": {},
   "source": [
    "## Semantic Search using Elasticsearch ELSER v2 and Llama3\n",
    "\n",
    "We will perform a semantic search on query with `ELSER v2` as the model. The contextually relevant answer is then composed into a template along with the users original query. \n",
    "\n",
    "We then user `Llama3` to answer your questions with contextually relevant data fetched earlier from Elasticsearch using the retriever.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d348766295f19e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, the organization's sales goals for fiscal year 2024 are:\n",
      "\n",
      "1. Increase revenue by 20% compared to fiscal year 2023.\n",
      "2. Expand market share in key segments by 15%.\n",
      "3. Retain 95% of existing customers and increase customer satisfaction ratings.\n",
      "4. Launch at least two new products or services in high-demand market segments.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "retriever = es_vector_store.as_retriever()\n",
    "template = \"\"\"Answer the question based only on the following context:\\n\n",
    "\n",
    "                {context}\n",
    "                \n",
    "                Question: {question}\n",
    "               \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "a = chain.invoke(\"What are the organizations sales goals?\")\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e855e64da97b052",
   "metadata": {},
   "source": [
    "_You could now try experimenting with other questions._\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
